{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc602ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application of knowledge learned through PyTorch.com Pytorch Turorials\n",
    "#Dataset from Kaggle, shoe images\n",
    "#Remove all .DS_store files in folders, ls-a\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "#Torch, Torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "\n",
    "##Image import settings\n",
    "datafolder_name = \"shoedata\"\n",
    "datafolder_name_use = datafolder_name + \"use\"\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "#(*TODO*) Normalize relative to image dataset being used\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(240),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(240),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(240),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproccessing and grouping all images\n",
    "reset_img_folder = True\n",
    "if reset_img_folder:\n",
    "    try:\n",
    "        print(\"removing used folder\")\n",
    "        !rm -rf {datafolder_name_use}\n",
    "        print(\"duplicating original folder\")\n",
    "        !cp -r {datafolder_name} {datafolder_name_use}\n",
    "    except FileNotFoundError as e:\n",
    "        !cp -r {datafolder_name} {datafolder_name_use}\n",
    "        print(\"duplicating original folder\")\n",
    "    \n",
    "    \n",
    "folders = [\"train\", \"val\", \"test\"]\n",
    "try:\n",
    "    for folder in folders:\n",
    "        \n",
    "        ds_store_path = os.path.join(datafolder_name_use, folder, \".DS_Store\")\n",
    "        print(ds_store_path)\n",
    "        !rm {ds_store_path}\n",
    "\n",
    "        i = 0\n",
    "        #extract images from category folders\n",
    "        for brand in ['adidas', 'converse', 'nike']:\n",
    "            for imgpath in os.listdir(os.path.join(datafolder_name_use,folder,brand)):\n",
    "                if imgpath == \".DS_Store\":\n",
    "                    ds_store_path = os.path.join(datafolder_name_use, folder,brand, \".DS_Store\")\n",
    "                    !rm {ds_store_path}\n",
    "                    print(ds_store_path)\n",
    "                    continue\n",
    "                os.rename(\n",
    "                        os.path.join(datafolder_name_use,folder,brand,imgpath),\n",
    "\n",
    "                        os.path.join(datafolder_name_use,folder,brand +\"_\"+ str(i) + \".png\")\n",
    "\n",
    "                         )\n",
    "                i+=1\n",
    "            os.rmdir(os.path.join(datafolder_name_use,folder,brand))\n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a78f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class for an image folder containing images of format (label_idx.jpg)\n",
    "#idx should be unique within a label\n",
    "class ShoeDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "        self.img_classes = class_list\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, os.listdir(self.img_dir)[idx])\n",
    "        image = Image.open(img_path)\n",
    "        label = torch.tensor(class_names[(img_path.split(\"_\")[0].split(\"/\")[-1])])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "#Index ordering does not matter\n",
    "#for each new class of shoe:\n",
    "    #add the name of the folder, and increasing index\n",
    "\n",
    "class_names = {\n",
    "    'adidas':0,\n",
    "    'converse':1,\n",
    "    'nike':2\n",
    "    #'reebok':3, \n",
    "}\n",
    "    \n",
    "class_list = [key for key in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f92dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing on real world\n",
    "def get_batch_size(folder, test_batch_size = 4):\n",
    "    if folder == 'test':\n",
    "        return None\n",
    "    else:\n",
    "        return 8\n",
    "    \n",
    "\n",
    "#have train, val\n",
    "#test from my own camera pictures\n",
    "data_dir = datafolder_name_use\n",
    "\n",
    "image_datasets = {x: ShoeDataset(os.path.join(data_dir, x) ,data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=get_batch_size(x, test_batch_size=1),\n",
    "                             shuffle= True)\n",
    "              for x in ['train', 'val','test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64521bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b744bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "#                 imshow(inputs[0])\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs.float())\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "   \n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.float() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288bcd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(weights = models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_list))\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "#Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Optimizer on model parameters\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model for num_epochs\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # default `log_dir` is \"runs\"\n",
    "writer = SummaryWriter('runs/shoeclass_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a04c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional - launch tensorboard: tensorboard --logdir=runs\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# # Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title = [class_list[int(x)] for x in classes])\n",
    "\n",
    "# # write to tensorboard\n",
    "writer.add_image('shoe_images', out)\n",
    "\n",
    "#write the final model to tensorboard\n",
    "writer.add_graph(model_ft.cpu(), inputs.cpu())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project images/features into 3D space on tensorboard, I like TSNE\n",
    "#higher quality or more photos may take the algorithm slightly longer, this is unmeasured\n",
    "#so far, only green channel is being looked at\n",
    "#(*TODO*) include all three channels\n",
    "\n",
    "# num_images_to_sample = 100\n",
    "# # select random images and their target indices\n",
    "# data = [image_datasets['train'][x] for x in range(0,num_images_to_sample)]\n",
    "\n",
    "# # get the class labels for each image\n",
    "# class_labels = [class_list[int(lab[1])] for lab in data[0:num_images_to_sample]]\n",
    "\n",
    "# images = torch.stack([img[0][1] for img in data[0:num_images_to_sample]])\n",
    "\n",
    "# print(images.shape)\n",
    "# # print(images.unsqueeze(-1).shape)\n",
    "# features = images.view(-1, 240 * 240)\n",
    "# print(features.shape)\n",
    "\n",
    "# # writer.add_embedding(features,\n",
    "#                     metadata=class_labels,\n",
    "#                     label_img=images.unsqueeze(1))\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737efbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
    "            print(class_list[labels.int()])\n",
    "            imshow(inputs)\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs.unsqueeze(0))\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "\n",
    "            images_so_far += 1\n",
    "# \n",
    "            print(f'predicted^^^ {class_list[preds[0]]}\\n\\n')\n",
    "\n",
    "            if images_so_far == dataset_sizes['test']:\n",
    "                model.train(mode=was_training)\n",
    "            \n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ec76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb51fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
